apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: wide-ep-llm-d-decode
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: decode
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: decode
      spec:
        serviceAccountName: deepseek-r1
        initContainers:
          - name: routing-proxy
            args:
              - --port=8000
              - --vllm-port=8200
              - --connector=nixlv2
              - -v=1
              - --secure-proxy=false
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
            imagePullPolicy: Always
            ports:
              - containerPort: 8000
            resources: {}
            restartPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 2Gi # 2Gi for NCCL 2.27 to be safe
        containers:
        - name: vllm-worker
          image: ghcr.io/llm-d/llm-d-cuda-dev:pr-230@sha256:744738e06eabc2dbf53b299d01572f149a25c752d31e7dd8bb828776c509076c
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
            runAsGroup: 0
            runAsUser: 0
          imagePullPolicy: Always
          command:
            - /bin/bash
            - -c
          args:
            - |-
              #################
              # RUN vLLM decode worker
              #################
              set -euo pipefail
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
            
              source /opt/vllm/bin/activate
              exec vllm serve \
                deepseek-ai/DeepSeek-R1-0528 \
                --port 8200 \
                --trust-remote-code \
                --disable-uvicorn-access-log \
                # Allow exernal load balancing across nodes, \
                # internal load balancing within a node \
                --data-parallel-hybrid-lb \
                # Use TPxDP in attention, EP in MoE layers \
                --enable-expert-parallel \
                --tensor-parallel-size $TP_SIZE \
                --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
                --data-parallel-size-local $DP_SIZE_LOCAL \
                --data-parallel-address ${LWS_LEADER_ADDRESS} \
                --data-parallel-rpc-port 5555 \
                --data-parallel-start-rank $START_RANK \
                # Used for P/D disaggregation \
                --kv_transfer_config '{"kv_connector":"NixlConnector",
                                        "kv_role":"kv_both"}' \
                # Reduce white space between engine steps \
                --async-scheduling \
                # Dual batch overlap (DBO) overlaps compute with collective communication. \
                --enable-dbo \
                --dbo-decode-token-threshold 32 \
                # Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts \
                # Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU \
                # Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be \
                # divisible by the number of GPUs. \
                --enable-eplb \
                --eplb-config '{"window_size":"1000",
                                "step_interval":"3000",
                                "num_redundant_experts":"32",
                                "log_balancedness":"False"}' \
                --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}'
          env:
          # VLLM_MOE_DP_CHUNK_SIZE is a tunable parameter.
          # Higher values increase the memory footprint of the hidden states in the MoE layers,
          # which decreases the available memory for KV Cache.
          # Lower values may cause poor performance, especially in load-imbalanced scenarios.
          # The value 512 was chosen to be greater than the concurrency expected to see on any DP rank.
          # Has no effect when VLLM_ALL2ALL_BACKEND=deepep_high_throughput.
            value: deepep_low_latency
          - name: VLLM_MOE_DP_CHUNK_SIZE
            value: "512" # vLLM default is 256
          - name: DP_SIZE_LOCAL
            value: "8"
          - name: TRITON_LIBCUDA_PATH
            value: /usr/lib64
          - name: VLLM_SKIP_P2P_CHECK
            value: "1"
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: deepep_low_latency
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          ports:
          - containerPort: 8200
            name: metrics
            protocol: TCP
          readinessProbe:
            httpGet:
              path: /health
              port: 8200
          resources:
            limits:
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: "8"
            requests:
              cpu: 32
              ephemeral-storage: 64Gi
              memory: 512Gi
              nvidia.com/gpu: "8"
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
