apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefill
spec:
  replicas: 4
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: Llama-33-70B-Instruct
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: Llama-33-70B-Instruct
        llm-d.ai/role: prefill
    spec:
      initContainers:
        - name: create-cufile-on-node # this init container will create the cufile on the node and runs before vllm
          image: quay.io/grpereir/amg-utils:latest # using my temporary image
          command: # k8s command overwrites docker entrypoint
            - /usr/local/bin/entrypoint.sh
          args: # k8s args overwrites docker command
            - /bin/bash
            - -lc
            - true
      serviceAccountName: llama-3-3-70b-instruct-fp8-dynamic
      volumes:
        - name: weka-storage # gets patched in overlays
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
        - name: host-node-cufile
          hostPath:
            path: ~/amg_stable/cufile.json # Load the file on Host (~/amg_stable/cufile.json)
            type: File 
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-dev:pr-188 # NEEDS TO BE UPDATED WITH AN IMAGE WITH UPSTREAM LMCACHE + EMBEDDED READINESS PROBE SCRIPT
          command: ["vllm", "serve"]
          args:
            - RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
            - --port
            - "8000"
            - --served-model-name
            - "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
            - --block-size
            - "128"
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
            - --disable-log-requests
            - --disable-uvicorn-access-log
            - --max-model-len
            - "32000"
          env:
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_LOGGING_LEVEL
              value: INFO # DEBUG IF NEEDED
            - name: HF_HOME # LOAD MODEL FROM WEKA MOUNT POINT
              value: "/mnt/weka/hf_cache"
            - name: NCCL_SOCKET_IFNAME
              value: "=enp157s0np0,enp158s0np0"
            - name: NCCL_IB_HCA
              value: "ibp"
            - name: UCX_NET_DEVICES
              value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NVIDIA_GDRCOPY
              value: "enabled"
          ports:
          - containerPort: 8000
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: "8"
              memory: 64Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
            requests:
              cpu: "8"
              memory: 64Gi
              nvidia.com/gpu: "1"
              rdma/ib: 1
          readinessProbe: # KUSTOMIZATION overlay
            exec:
              command:
                - "gds-cufile-probe" # Use default /etc/cufile.json in container, no path necessary
            initialDelaySeconds: 15
            periodSeconds: 10
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: weka-storage
              mountPath: /mnt/weka
            - name: host-node-cufile
              mountPath: /etc/cufile.json # Mount from ~/amg_stable/cufile.json to /etc/cufile.json in container
              mountPropagation: HostToContainer
              readOnly: true # is this acurate? Not required
